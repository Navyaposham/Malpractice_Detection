{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to C:\\Users\\posha\\OneDrive\\Malpractices\\archive\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the ZIP file\n",
        "zip_path = r\"C:\\Users\\posha\\OneDrive\\Malpractices\\archive (4) - Copy.zip\"\n",
        "\n",
        "# Folder to extract to\n",
        "extract_path = r\"C:\\Users\\posha\\OneDrive\\Malpractices\\archive\"\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Extracted to {os.path.abspath(extract_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch) (2025.7.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tqdm) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (0.22.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torchvision) (2.1.3)\n",
            "Requirement already satisfied: torch==2.7.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torchvision) (2.7.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch==2.7.1->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch==2.7.1->torchvision) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch==2.7.1->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch==2.7.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch==2.7.1->torchvision) (2025.7.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from torch==2.7.1->torchvision) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py egg_info did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [15 lines of output]\n",
            "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "      rather than 'sklearn' for pip commands.\n",
            "      \n",
            "      Here is how to fix this error in the main use cases:\n",
            "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "      - if the 'sklearn' package is used by one of your dependencies,\n",
            "        it would be great if you take some time to track which package uses\n",
            "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "      - as a last resort, set the environment variable\n",
            "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "      \n",
            "      More information is available at\n",
            "      https://github.com/scikit-learn/sklearn-pypi-package\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Batches: 88\n",
            "Val Batches: 10\n",
            "Test Batches: 496\n",
            "Classes: {'cheating': 0, 'giving code': 1, 'giving object': 2, 'looking friend': 3, 'normal act': 4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Training: 100%|██████████| 88/88 [00:33<00:00,  2.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.1296, Accuracy: 0.7018\n",
            "Epoch 1, Val Loss: 0.9549, Accuracy: 0.8599\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 - Training: 100%|██████████| 88/88 [00:22<00:00,  3.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 0.4984, Accuracy: 0.8548\n",
            "Epoch 2, Val Loss: 0.3405, Accuracy: 0.9236\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 - Training: 100%|██████████| 88/88 [00:21<00:00,  4.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 0.2490, Accuracy: 0.9132\n",
            "Epoch 3, Val Loss: 0.3542, Accuracy: 0.9172\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 - Training: 100%|██████████| 88/88 [00:20<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 0.1360, Accuracy: 0.9416\n",
            "Epoch 4, Val Loss: 0.3335, Accuracy: 0.9363\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 - Training: 100%|██████████| 88/88 [00:21<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Train Loss: 0.0977, Accuracy: 0.9601\n",
            "Epoch 5, Val Loss: 0.6598, Accuracy: 0.9363\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision import transforms as T\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Custom Dataset\n",
        "# ----------------------\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, data_type=None, transformations=None):\n",
        "        self.transformations = transformations\n",
        "        self.data_type = data_type\n",
        "        if data_type == \"train\":\n",
        "            self.im_paths = glob(f\"{root}/train/*/*.*\")\n",
        "            self.get_info()\n",
        "        elif data_type == \"test\":\n",
        "            self.im_paths = glob(f\"{root}/test/**/*.*\", recursive=True)\n",
        "        else:\n",
        "            raise ValueError(\"data_type must be 'train' or 'test'\")\n",
        "\n",
        "    def get_info(self):\n",
        "        self.cls_names, self.cls_counts, count = {}, {}, 0\n",
        "        for im_path in self.im_paths:\n",
        "            class_name = os.path.basename(os.path.dirname(im_path))\n",
        "            if class_name not in self.cls_names:\n",
        "                self.cls_names[class_name] = count\n",
        "                self.cls_counts[class_name] = 1\n",
        "                count += 1\n",
        "            else:\n",
        "                self.cls_counts[class_name] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        im_path = self.im_paths[idx]\n",
        "        im = Image.open(im_path).convert(\"RGB\")\n",
        "        if self.data_type == \"train\":\n",
        "            label_name = os.path.basename(os.path.dirname(im_path))\n",
        "            gt = self.cls_names[label_name]\n",
        "        else:\n",
        "            gt = im_path  # For inference\n",
        "        if self.transformations:\n",
        "            im = self.transformations(im)\n",
        "        return im, gt\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Data Loaders\n",
        "# ----------------------\n",
        "def get_dls(root, transformations, bs, split=[0.9, 0.1], ns=0):\n",
        "    dataset = CustomDataset(root=root, data_type=\"train\", transformations=transformations)\n",
        "    ts_ds = CustomDataset(root=root, data_type=\"test\", transformations=transformations)\n",
        "\n",
        "    tr_len = int(len(dataset) * split[0])\n",
        "    vl_len = len(dataset) - tr_len\n",
        "\n",
        "    tr_ds, vl_ds = random_split(dataset, [tr_len, vl_len])\n",
        "\n",
        "    tr_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True, num_workers=ns)\n",
        "    val_dl = DataLoader(vl_ds, batch_size=bs, shuffle=False, num_workers=ns)\n",
        "    ts_dl = DataLoader(ts_ds, batch_size=1, shuffle=False, num_workers=ns)\n",
        "\n",
        "    return tr_dl, val_dl, ts_dl, dataset.cls_names\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Custom CNN Model\n",
        "# ----------------------\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Training Pipeline\n",
        "# ----------------------\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=25):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {total_loss/total:.4f}, Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * imgs.size(0)\n",
        "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss/val_total:.4f}, Accuracy: {val_acc:.4f}\\n\")\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Setup & Run Training\n",
        "# ----------------------\n",
        "root = r\"C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\ExamCheatingDataset\"\n",
        "mean, std, im_size = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 64\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize((im_size, im_size)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "tr_dl, val_dl, ts_dl, classes = get_dls(root=root, transformations=transform, bs=16, ns=0)\n",
        "\n",
        "print(\"Train Batches:\", len(tr_dl))\n",
        "print(\"Val Batches:\", len(val_dl))\n",
        "print(\"Test Batches:\", len(ts_dl))\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Run Training\n",
        "# ----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomCNN(num_classes=len(classes))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "class_weights = compute_class_weight('balanced', classes=np.arange(len(classes)), y=[label for _, label in tr_dl.dataset])\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "\n",
        "train_model(model, tr_dl, val_dl, criterion, optimizer, device, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (3.10.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ARcicV3Lngqx",
        "outputId": "c1e779a2-f14c-421d-f33a-94b4b87af696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Batches: 340\n",
            "Val Batches: 38\n",
            "Test Batches: 496\n",
            "Classes: {'cheating': 0, 'giving code': 1, 'giving object': 2, 'looking friend': 3, 'normal act': 4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Training: 100%|██████████| 340/340 [00:31<00:00, 10.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.5003, Cls Acc: 0.5099, Spoof Acc: 0.6706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 - Training: 100%|██████████| 340/340 [00:20<00:00, 16.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 1.1432, Cls Acc: 0.6699, Spoof Acc: 0.7406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 - Training: 100%|██████████| 340/340 [00:17<00:00, 19.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Loss: 0.8648, Cls Acc: 0.7443, Spoof Acc: 0.7907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 - Training: 100%|██████████| 340/340 [00:16<00:00, 20.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Loss: 0.6915, Cls Acc: 0.8099, Spoof Acc: 0.8254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 - Training: 100%|██████████| 340/340 [00:25<00:00, 13.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Loss: 0.5843, Cls Acc: 0.8467, Spoof Acc: 0.8651\n",
            "✅ Training Complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms as T\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Custom Dataset Loader\n",
        "# ----------------------\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, data_type=None, transformations=None):\n",
        "        self.transformations = transformations\n",
        "        self.data_type = data_type\n",
        "        if data_type == \"train\":\n",
        "            self.im_paths = glob(os.path.join(root, \"train\", \"*\", \"*.jpg\"))\n",
        "            self.get_info()\n",
        "        elif data_type == \"test\":\n",
        "            self.im_paths = glob(os.path.join(root, \"test\", \"**\", \"*.jpg\"), recursive=True)\n",
        "        else:\n",
        "            raise ValueError(\"data_type must be 'train' or 'test'\")\n",
        "\n",
        "    def get_info(self):\n",
        "        self.cls_names, self.cls_counts, count = {}, {}, 0\n",
        "        for im_path in self.im_paths:\n",
        "            class_name = os.path.basename(os.path.dirname(im_path))\n",
        "            if class_name not in self.cls_names:\n",
        "                self.cls_names[class_name] = count\n",
        "                self.cls_counts[class_name] = 1\n",
        "                count += 1\n",
        "            else:\n",
        "                self.cls_counts[class_name] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        im_path = self.im_paths[idx]\n",
        "        im = Image.open(im_path).convert(\"RGB\")\n",
        "\n",
        "        if self.data_type == \"train\":\n",
        "            label_name = os.path.basename(os.path.dirname(im_path))\n",
        "            gt = self.cls_names[label_name]\n",
        "            spoof_label = 0 if label_name.lower() == \"normal act\" else 1\n",
        "            if self.transformations:\n",
        "                im = self.transformations(im)\n",
        "            return im, gt, spoof_label\n",
        "        else:\n",
        "            if self.transformations:\n",
        "                im = self.transformations(im)\n",
        "            return im, im_path  # For test, return image & path\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Data Loaders\n",
        "# ----------------------\n",
        "def get_dls(root, transformations, bs, split=[0.9, 0.1], ns=0):\n",
        "    dataset = CustomDataset(root=root, data_type=\"train\", transformations=transformations)\n",
        "    ts_ds = CustomDataset(root=root, data_type=\"test\", transformations=transformations)\n",
        "\n",
        "    tr_len = int(len(dataset) * split[0])\n",
        "    vl_len = len(dataset) - tr_len\n",
        "\n",
        "    tr_ds, vl_ds = random_split(dataset, [tr_len, vl_len])\n",
        "\n",
        "    tr_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True, num_workers=ns)\n",
        "    val_dl = DataLoader(vl_ds, batch_size=bs, shuffle=False, num_workers=ns)\n",
        "    ts_dl = DataLoader(ts_ds, batch_size=1, shuffle=False, num_workers=ns)\n",
        "\n",
        "    return tr_dl, val_dl, ts_dl, dataset.cls_names\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Lightweight CNN with Adaptive Pooling\n",
        "# ----------------------\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 64)\n",
        "        self.fc_cls = nn.Linear(64, num_classes)\n",
        "        self.fc_spoof = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        cls_out = self.fc_cls(x)\n",
        "        spoof_out = self.fc_spoof(x)\n",
        "        return cls_out, spoof_out\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Training Function\n",
        "# ----------------------\n",
        "def train_model(model, train_loader, criterion_cls, criterion_spoof, optimizer, device, epochs=3):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct_cls, correct_spoof, total = 0.0, 0, 0, 0\n",
        "\n",
        "        for imgs, labels, spoof_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "            imgs, labels, spoof_labels = imgs.to(device), labels.to(device), spoof_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out_cls, out_spoof = model(imgs)\n",
        "\n",
        "            loss_cls = criterion_cls(out_cls, labels)\n",
        "            loss_spoof = criterion_spoof(out_spoof, spoof_labels)\n",
        "            loss = loss_cls + 0.5 * loss_spoof\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            correct_cls += (out_cls.argmax(1) == labels).sum().item()\n",
        "            correct_spoof += (out_spoof.argmax(1) == spoof_labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc_cls = correct_cls / total\n",
        "        train_acc_spoof = correct_spoof / total\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/total:.4f}, Cls Acc: {train_acc_cls:.4f}, Spoof Acc: {train_acc_spoof:.4f}\")\n",
        "\n",
        "# ----------------------\n",
        "# ✅ Main Script Execution\n",
        "# ----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    root = r\"C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\ExamCheatingDataset\"\n",
        "    mean, std, im_size = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 112  # Reduced image size\n",
        "\n",
        "    transform = T.Compose([\n",
        "        T.Resize((im_size, im_size)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    tr_dl, val_dl, ts_dl, classes = get_dls(root=root, transformations=transform, bs=4, ns=0)\n",
        "\n",
        "    print(f\"Train Batches: {len(tr_dl)}\")\n",
        "    print(f\"Val Batches: {len(val_dl)}\")\n",
        "    print(f\"Test Batches: {len(ts_dl)}\")\n",
        "    print(f\"Classes: {classes}\")\n",
        "\n",
        "    # Compute class weights\n",
        "    cls_counts = {v: 0 for v in range(len(classes))}\n",
        "    for idx in tr_dl.dataset.indices:\n",
        "        path = tr_dl.dataset.dataset.im_paths[idx]\n",
        "        class_name = os.path.basename(os.path.dirname(path))\n",
        "        cls_idx = tr_dl.dataset.dataset.cls_names[class_name]\n",
        "        cls_counts[cls_idx] += 1\n",
        "\n",
        "    all_labels = []\n",
        "    for cls_idx, count in cls_counts.items():\n",
        "        all_labels.extend([cls_idx] * count)\n",
        "\n",
        "    class_weights = compute_class_weight('balanced', classes=np.arange(len(classes)), y=all_labels)\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SmallCNN(num_classes=len(classes))\n",
        "    criterion_cls = nn.CrossEntropyLoss(weight=weights_tensor.to(device))\n",
        "    criterion_spoof = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "    train_model(model, tr_dl, criterion_cls, criterion_spoof, optimizer, device, epochs=5)\n",
        "\n",
        "    print(\"✅ Training Complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_jRlUinckXGR"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"custom_cnn_full_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "bzbE33zMtZ94",
        "outputId": "45368822-286a-4c82-cacf-c6d7037fe878"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 38/38 [00:01<00:00, 22.69it/s]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      cheating       0.00      0.00      0.00         3\n",
            "   giving code       0.83      1.00      0.91        29\n",
            " giving object       0.00      0.00      0.00         2\n",
            "looking friend       0.93      0.97      0.95        58\n",
            "    normal act       0.94      0.85      0.89        59\n",
            "\n",
            "      accuracy                           0.89       151\n",
            "     macro avg       0.54      0.56      0.55       151\n",
            "  weighted avg       0.89      0.89      0.89       151\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\posha\\OneDrive\\Malpractices\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\posha\\OneDrive\\Malpractices\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\posha\\OneDrive\\Malpractices\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, loader, class_dict, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _ in tqdm(loader, desc=\"Testing\"):  # ✅ FIXED: unpack 3 values\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            cls_outputs, _ = model(images)  # ✅ model returns (cls_out, spoof_out)\n",
        "            preds = cls_outputs.argmax(1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    class_names = list(class_dict.keys())\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "# ✅ Run Evaluation\n",
        "evaluate_model(model, val_dl, classes, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from seaborn) (2.1.3)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from seaborn) (2.3.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from seaborn) (3.10.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DEEj6FDiuvqm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def predict_on_test(model, test_loader, class_dict, device, save_path=\"test_predictions_with_spoof.csv\"):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    idx_to_class = {v: k for k, v in class_dict.items()}  # map index → class name\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, paths in tqdm(test_loader, desc=\"Predicting\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Model returns (cls_output, spoof_output)\n",
        "            cls_outputs, spoof_outputs = model(images)\n",
        "\n",
        "            # Predicted class index & class name\n",
        "            preds = cls_outputs.argmax(1).cpu().numpy()\n",
        "\n",
        "            # Spoof confidence score (Probability of being Spoof)\n",
        "            spoof_probs = torch.softmax(spoof_outputs, dim=1)[:, 1].cpu().numpy()  # spoof_label index=1\n",
        "\n",
        "            for path, pred, spoof_score in zip(paths, preds, spoof_probs):\n",
        "                results.append({\n",
        "                    \"image_path\": path,\n",
        "                    \"predicted_class\": idx_to_class[pred],\n",
        "                    \"predicted_index\": int(pred),\n",
        "                    \"spoof_confidence\": float(spoof_score)  # Between 0 to 1\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"✅ Predictions with Anti-Spoofing Scores saved to {save_path}\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvgUiHVxuycV",
        "outputId": "e1aebfe2-15d4-4610-b57d-8f2051c6089e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 496/496 [00:15<00:00, 32.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Predictions with Anti-Spoofing Scores saved to test_predictions_with_spoof.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>predicted_class</th>\n",
              "      <th>predicted_index</th>\n",
              "      <th>spoof_confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...</td>\n",
              "      <td>cheating</td>\n",
              "      <td>0</td>\n",
              "      <td>0.250312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...</td>\n",
              "      <td>cheating</td>\n",
              "      <td>0</td>\n",
              "      <td>0.933924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...</td>\n",
              "      <td>cheating</td>\n",
              "      <td>0</td>\n",
              "      <td>0.839507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...</td>\n",
              "      <td>cheating</td>\n",
              "      <td>0</td>\n",
              "      <td>0.839208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...</td>\n",
              "      <td>cheating</td>\n",
              "      <td>0</td>\n",
              "      <td>0.664523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          image_path predicted_class  \\\n",
              "0  C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...        cheating   \n",
              "1  C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...        cheating   \n",
              "2  C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...        cheating   \n",
              "3  C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...        cheating   \n",
              "4  C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\E...        cheating   \n",
              "\n",
              "   predicted_index  spoof_confidence  \n",
              "0                0          0.250312  \n",
              "1                0          0.933924  \n",
              "2                0          0.839507  \n",
              "3                0          0.839208  \n",
              "4                0          0.664523  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run Inference with Spoofing Score\n",
        "predict_df = predict_on_test(model, ts_dl, classes, device)\n",
        "predict_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.11.1)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
            "Requirement already satisfied: namex in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (25.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install tensorflow --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from opencv-python) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.11.0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "print(cv2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.11.1)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
            "Requirement already satisfied: namex in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\posha\\onedrive\\malpractices\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes found: ['cheating', 'giving code', 'giving object', 'looking friend', 'normal act']\n",
            "Processing class 'cheating' (label 0)\n",
            "Processing class 'giving code' (label 1)\n",
            "Processing class 'giving object' (label 2)\n",
            "Processing class 'looking friend' (label 3)\n",
            "Processing class 'normal act' (label 4)\n",
            "✅ Loaded 1562 images in total.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\posha\\OneDrive\\Malpractices\\myenv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n",
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 535ms/step - accuracy: 0.5685 - loss: 1.1883 - val_accuracy: 0.8626 - val_loss: 0.4897\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 486ms/step - accuracy: 0.8575 - loss: 0.3986 - val_accuracy: 0.9137 - val_loss: 0.2442\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 380ms/step - accuracy: 0.9055 - loss: 0.2614 - val_accuracy: 0.9233 - val_loss: 0.2077\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 441ms/step - accuracy: 0.9496 - loss: 0.1467 - val_accuracy: 0.9297 - val_loss: 0.1726\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 401ms/step - accuracy: 0.9704 - loss: 0.0966 - val_accuracy: 0.9425 - val_loss: 0.1630\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 322ms/step - accuracy: 0.9760 - loss: 0.0855 - val_accuracy: 0.9425 - val_loss: 0.1458\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 386ms/step - accuracy: 0.9848 - loss: 0.0675 - val_accuracy: 0.9489 - val_loss: 0.1361\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 392ms/step - accuracy: 0.9816 - loss: 0.0574 - val_accuracy: 0.9393 - val_loss: 0.1854\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 385ms/step - accuracy: 0.9816 - loss: 0.0551 - val_accuracy: 0.9489 - val_loss: 0.1524\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 333ms/step - accuracy: 0.9888 - loss: 0.0363 - val_accuracy: 0.9457 - val_loss: 0.1856\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: C:\\Users\\posha\\OneDrive\\Malpractices\\cheating_detection_model.h5\n",
            "Class labels saved to: C:\\Users\\posha\\OneDrive\\Malpractices\\cheating_class_labels.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# === Parameters ===\n",
        "img_size = 128\n",
        "data_dir = r\"C:\\Users\\posha\\OneDrive\\Malpractices\\archive\\ExamCheatingDataset\\train\"  # path to training images\n",
        "save_dir = r\"C:\\Users\\posha\\OneDrive\\Malpractices\"  # where to save the model and labels\n",
        "\n",
        "classes = sorted(os.listdir(data_dir))  \n",
        "X, y = [], []\n",
        "\n",
        "print(\"Classes found:\", classes)\n",
        "\n",
        "for idx, class_name in enumerate(classes):\n",
        "    class_path = os.path.join(data_dir, class_name)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing class '{class_name}' (label {idx})\")\n",
        "\n",
        "    for img_name in os.listdir(class_path):\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        # Only process valid image files\n",
        "        if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Could not load image {img_path}\")\n",
        "                continue\n",
        "            img = cv2.resize(img, (img_size, img_size))\n",
        "            X.append(img)\n",
        "            y.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {img_path}: {e}\")\n",
        "\n",
        "# === Prepare data ===\n",
        "X = np.array(X) / 255.0  # Normalize\n",
        "if len(y) == 0:\n",
        "    raise ValueError(\"No valid images found!\")\n",
        "\n",
        "y = to_categorical(y)  # One-hot encode\n",
        "\n",
        "print(f\"✅ Loaded {len(X)} images in total.\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# === Build CNN model ===\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(len(classes), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print(\"Training started...\")\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "\n",
        "model_path = os.path.join(save_dir, \"cheating_detection_model.h5\")\n",
        "label_path = os.path.join(save_dir, \"cheating_class_labels.txt\")\n",
        "\n",
        "model.save(model_path)\n",
        "with open(label_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(classes))\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Class labels saved to: {label_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Press 'q' to quit.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 18.24 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 13.90 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 17.55 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 14.33 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 29.05 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 19.35 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 18.57 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 17.23 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 24.02 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 38.41 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 25.67 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 13.85 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 27.20 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 12.98 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 17.47 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 20.22 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 14.94 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 17.69 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 17.92 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 27.01 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 10.92 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 13.38 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "[MOVEMENT DETECTED] Frame Change: 22.83 → Simulated FAKE\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import mediapipe as mp\n",
        "\n",
        "# Load Cheating Detection Model\n",
        "cheating_model = tf.keras.models.load_model(r\"C:\\Users\\posha\\OneDrive\\Malpractices\\cheating_detection_model.h5\")\n",
        "with open(r\"C:\\Users\\posha\\OneDrive\\Malpractices\\cheating_class_labels.txt\", \"r\") as f:\n",
        "    cheating_labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Load Anti-Spoofing Model\n",
        "spoof_model = tf.keras.models.load_model(r\"C:\\Users\\posha\\OneDrive\\Malpractices\\anti_spoofing_model.h5\")\n",
        "with open(r\"C:\\Users\\posha\\OneDrive\\Malpractices\\anti_spoofing_labels.txt\", \"r\") as f:\n",
        "    spoof_labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Initialize MediaPipe Face Detection\n",
        "mp_face = mp.solutions.face_detection\n",
        "face_detection = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.6)\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Webcam not detected.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Press 'q' to quit.\")\n",
        "\n",
        "prev_gray = None  # Store previous grayscale frame\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Compute Frame Difference\n",
        "    frame_diff = 0\n",
        "    if prev_gray is not None:\n",
        "        diff = cv2.absdiff(prev_gray, frame_gray)\n",
        "        frame_diff = np.sum(diff) / (frame_gray.shape[0] * frame_gray.shape[1])\n",
        "\n",
        "    prev_gray = frame_gray.copy()\n",
        "\n",
        "    # Face Detection\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_detection.process(frame_rgb)\n",
        "\n",
        "    if results.detections:\n",
        "        for detection in results.detections:\n",
        "            bboxC = detection.location_data.relative_bounding_box\n",
        "            ih, iw, _ = frame.shape\n",
        "            x = int(bboxC.xmin * iw)\n",
        "            y = int(bboxC.ymin * ih)\n",
        "            w = int(bboxC.width * iw)\n",
        "            h = int(bboxC.height * ih)\n",
        "\n",
        "            # Clamp coordinates to frame bounds\n",
        "            x = max(0, x)\n",
        "            y = max(0, y)\n",
        "            x2 = min(iw, x + w)\n",
        "            y2 = min(ih, y + h)\n",
        "\n",
        "            # Skip invalid crops\n",
        "            if x2 - x <= 0 or y2 - y <= 0:\n",
        "                continue\n",
        "\n",
        "            face_crop = frame[y:y2, x:x2]\n",
        "\n",
        "            if face_crop.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Preprocess for Anti-Spoofing\n",
        "            spoof_img = cv2.resize(face_crop, (128, 128))\n",
        "            spoof_img = img_to_array(spoof_img) / 255.0\n",
        "            spoof_img = np.expand_dims(spoof_img, axis=0)\n",
        "            spoof_pred = spoof_model.predict(spoof_img)[0]\n",
        "            spoof_idx = np.argmax(spoof_pred)\n",
        "            spoof_label = spoof_labels[spoof_idx]\n",
        "            spoof_confidence = spoof_pred[spoof_idx] * 100\n",
        "\n",
        "            if frame_diff > 10:  \n",
        "                if spoof_label == 'Real':\n",
        "                    spoof_label = 'Fake'\n",
        "                    spoof_confidence = 95.0\n",
        "                    print(f\"[MOVEMENT DETECTED] Frame Change: {frame_diff:.2f} → Simulated FAKE\")\n",
        "\n",
        "            # Draw bounding box\n",
        "            color = (0, 255, 0) if spoof_label == 'Real' else (0, 0, 255)\n",
        "            cv2.rectangle(frame, (x, y), (x2, y2), color, 2)\n",
        "\n",
        "            # Display Spoof Result\n",
        "            spoof_text = f\"Spoof: {spoof_label} ({spoof_confidence:.2f}%)\"\n",
        "            cv2.putText(frame, spoof_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "    # Cheating Detection (Full Frame or ROI if required)\n",
        "    cheat_img = cv2.resize(frame, (128, 128))\n",
        "    cheat_img = img_to_array(cheat_img) / 255.0\n",
        "    cheat_img = np.expand_dims(cheat_img, axis=0)\n",
        "    cheat_pred = cheating_model.predict(cheat_img)[0]\n",
        "    cheat_idx = np.argmax(cheat_pred)\n",
        "    cheat_label = cheating_labels[cheat_idx]\n",
        "    cheat_confidence = cheat_pred[cheat_idx] * 100\n",
        "\n",
        "    cheat_text = f\"Cheating: {cheat_label} ({cheat_confidence:.2f}%)\"\n",
        "    cv2.putText(frame, cheat_text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "\n",
        "    cv2.imshow(\"Cheating & Anti-Spoofing Detection\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
